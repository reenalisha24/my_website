---
categories:  
- ""    #the front matter should be like the one found in, e.g., blog2.md. It cannot be like the normal Rmd we used
- ""
date: "2021-10-03"
description: Analysis of London Bike Rentals # the title that will show up once someone gets to this page
draft: false
#image: spices.jpg # save picture in \static\img\blogs. Acceptable formats= jpg, jpeg, or png . Your iPhone pics wont work

keywords: ""
slug: bike # slug is the shorthand URL address... no spaces plz
title: Monthly changes in bike rentals from 2016 -2021
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE,
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(fivethirtyeight)
library(vroom)
library(tidyquant)
library(rvest) # to scrape wikipedia page
```


We look at the TfL data on how many bikes were hired every single day. We can get the latest data by running the following

```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))
```

May and June of 2020 show unusually low levels of bike rentals throughout. This is most likely related to COVID-19, but without any further data, we cant say for sure and we dont want to bring our bias into the classroom, do we?

In this challenge, I will try to plot the monthly changes in bike rentals.

```{r}
# Compute the mean of number of bikes hired for each month during the time period 2016-19
bike_1619_mean <- 
  bike %>% 
  filter(year == c(2016:2019)) %>% 
  group_by(month) %>% 
  summarise(mean_1619 = mean(bikes_hired))

# Compute mean of number of bikes hired for each month in each year during the time period 2016-21
bike_monthly <-
  bike %>% 
  filter(year %in% c(2016:2021)) %>% 
  group_by(month,year) %>% 
  summarise(mean = mean(bikes_hired)) %>% 
  #mutate(delta = bikes_hired - mean) %>% 
  arrange(year,month)

# Join the two datasets to create dataset used in plotting, and add some features needed
bike_monthly_plot <- left_join(bike_monthly,bike_1619_mean,by = 'month') %>% 
  mutate(increase = ifelse(mean > mean_1619,mean-mean_1619,0),
         decrease = ifelse(mean < mean_1619,mean-mean_1619,0))

# Create the plot
bike_monthly_plot %>% 
  ggplot(aes(x = month)) +
  geom_ribbon(aes(ymin = mean_1619,
                  ymax = mean_1619 + increase),
                  fill = 'lightgreen',
              group=1) +
  geom_ribbon(aes(ymin = mean_1619 + decrease,ymax=mean_1619),fill = 'salmon',group=1) +
  geom_line(aes(y = mean_1619, group=1),color = 'blue',size = 1) +
  geom_line(aes(y = mean, group=1), color = 'grey30') +
  facet_wrap(~year, scales = "free") +
  labs(title = 'Monthly Changes in Tfl Bike Rentals', subtitle = 'Blue line represents monthly average from 2016-19', y = 'Bike Rentals', x = 'Month') +
  theme_bw() +
  NULL
```


The second challenge looks at percentage changes from the expected level of weekly rentals. The two grey shaded rectangles correspond to Q2 (weeks 14-26) and Q4 (weeks 40-52).

For both of these graphs, I have to calculate the expected number of rentals per week or month between 2016-2019 and then, see how each week/month of 2020-2021 compares to the expected rentals. Think of the calculation `excess_rentals = actual_rentals - expected_rentals`. 

```{r}
# Compute the mean of number of bikes hired for each week during the time period 2016-19
bike_1619_mean_weekly <- 
  bike %>% 
  filter(year == c(2016:2019)) %>% 
  group_by(week) %>% 
  summarise(mean_1619 = mean(bikes_hired))

# Compute mean of number of bikes hired for each week in each year during the time period 2016-21
bike_weekly <-
  bike %>% 
  filter(year %in% c(2016:2021)) %>% 
  group_by(week,year) %>% 
  summarise(mean = mean(bikes_hired)) %>% 
  #mutate(delta = bikes_hired - mean) %>% 
  arrange(year,week)

# Join the two datasets to create dataset used in plotting, and add some features needed
bike_weekly_plot <- left_join(bike_weekly,bike_1619_mean_weekly,by = 'week') %>% 
  mutate(percent_change = (mean - mean_1619)/mean_1619*100,
         percent_increase = ifelse(percent_change>0,percent_change,0),
         percent_decrease = ifelse(percent_change<0,percent_change,0),
         )

# create the plot
bike_weekly_plot %>% 
  ggplot(aes(x = week, y = percent_change)) +
  geom_rect(aes(xmin = 14,xmax = 26, ymin = -75, ymax = 125), fill = 'grey95') +
  geom_rect(aes(xmin = 40,xmax = 52, ymin = -75, ymax = 125), fill = 'grey95') +
  geom_line(group=1) +
  geom_area(aes(y = percent_increase),fill='lightgreen') +
  geom_area(aes(y = percent_decrease),fill='salmon') +
  geom_rug(sides = 'b',color = ifelse(bike_weekly_plot$percent_change>0,'lightgreen','salmon')) +
  labs(title = 'Weekly Changes in Tfl bike rentals', subtitle = '% change from 2016-19 weekly averages calculated between 2016-2019', x = 'Week',y = 'Percentage Change (%)', caption = "Source: TfL, London Data Store") +
  facet_wrap(~year, scales = "free") +
  theme_bw() +
  NULL
```


Qn: Should you use the mean or the median to calculate your expected rentals? Why?

The median should be the better predictor for the practical forecasting application. The mean is easily obscured by extreme values or outliers, and in an economic scenario we would most likely want to clean our prediction for that.
